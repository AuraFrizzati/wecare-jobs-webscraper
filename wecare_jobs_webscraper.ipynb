{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMZrLm1cdSBXlYK1EdUQUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AuraFrizzati/wecare-jobs-webscraper/blob/main/wecare_jobs_webscraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Webscraper for job adverts from \"wecare.wales\"**\n",
        "https://wecare.wales/jobs/results"
      ],
      "metadata": {
        "id": "fxCc_nVA6m93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load relevant libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import html\n",
        "import datetime\n",
        "\n",
        "# main webscraping function\n",
        "def scrape_wecare_jobs(url, page_num):\n",
        "  response = requests.get(url)\n",
        "  response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  job_articles = soup.find_all('article')\n",
        "  jobs_data = []  # List to store job data as dictionaries\n",
        "\n",
        "  for job_item in job_articles:\n",
        "\n",
        "\n",
        "    # extract title\n",
        "    title_element = job_item.find('h2', class_='font-h4')\n",
        "    title = title_element.text.strip()\n",
        "\n",
        "    # extract the closing date\n",
        "    closing_date_element = job_item.find('p', string=re.compile(r'Closing date:'))\n",
        "    if closing_date_element:\n",
        "        closing_date = closing_date_element.text.split(': ')[1].strip()\n",
        "    else:\n",
        "        closing_date = 'N/A'\n",
        "\n",
        "    # extract company, location, job type and salary\n",
        "    detail_items = job_item.find('ul', class_='list-none').find_all('li')\n",
        "    company = detail_items[0].text.strip()\n",
        "    location = detail_items[1].text.strip()\n",
        "    job_type = detail_items[2].text.strip()\n",
        "    salary = detail_items[3].text.strip().replace('Ã‚', '')\n",
        "\n",
        "    # extract job description\n",
        "    description_element = job_item.find('div', class_='small-text')\n",
        "    description = description_element.text.strip()\n",
        "\n",
        "    # extract job tag\n",
        "    job_tag = job_item.select('span.tag.meta.flex-shrink-0[class*=\"bg-\"]')\n",
        "    job_tag_text = ', '.join([tag.text.strip() for tag in job_tag])\n",
        "\n",
        "    # extract job link\n",
        "    job_details_link_element = job_item.find('div', class_='bg-cyan-20').find('a', class_='button')\n",
        "    link = job_details_link_element['href']\n",
        "\n",
        "    jobs_data.append({\n",
        "    'title': title\n",
        "    ,'closing_date': closing_date\n",
        "    ,'company': company\n",
        "    ,'location': location\n",
        "    ,'job_type': job_type\n",
        "    ,'salary': salary\n",
        "    ,'description': description\n",
        "    ,'Job_tag':  job_tag_text\n",
        "    ,'link_id': link\n",
        "    ,'page_scraped': page_num\n",
        "    ,'date_of_scraping': datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    })\n",
        "\n",
        "  return pd.DataFrame(jobs_data)\n",
        "\n",
        "## Initialise parameters and dataframe\n",
        "base_url = \"https://wecare.wales/jobs/results/\"\n",
        "max_jobs_per_page = 12\n",
        "all_jobs_df = pd.DataFrame()  # Initialize an empty DataFrame to store all jobs\n",
        "\n",
        "print(f\"webscraping of {base_url}\")\n",
        "\n",
        "# Get the total number of posted jobs in the day\n",
        "response = requests.get(base_url)\n",
        "response.raise_for_status()\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "total_jobs = int(soup.find('h1').text.split(' ')[0])  # h1 contains the total job count\n",
        "print(f\"There are a total of {total_jobs} job adverts posted\")\n",
        "\n",
        "# calculate the number of pages to retrieve\n",
        "num_pages = math.ceil(total_jobs / max_jobs_per_page)\n",
        "print(f\"There are {num_pages} pages to retrieve\")\n",
        "\n",
        "# Loop through each page\n",
        "print(\".....................\")\n",
        "print(\"Webscraping started\")\n",
        "\n",
        "all_jobs_df = pd.DataFrame()  # Initialize an empty DataFrame to store all jobs\n",
        "# for page_num in range(1, 3):\n",
        "for page_num in range(1, num_pages + 1):\n",
        "      page_url = f\"{base_url}p{page_num}\" if page_num > 1 else base_url\n",
        "      print(f\"Scraping data from: {page_url}\")\n",
        "      page_job_df = scrape_wecare_jobs(page_url, page_num)\n",
        "      all_jobs_df = pd.concat([all_jobs_df, page_job_df], ignore_index=True)\n",
        "\n",
        "print(\".....................\")\n",
        "print(\"Webscraping completed\")\n",
        "\n",
        "#all_jobs_df.head()\n",
        "\n",
        "## Convert dataframe to csv file a csv file\n",
        "\n",
        "print(\".....................\")\n",
        "print(\"Creation of CSV file\")\n",
        "\n",
        "if not all_jobs_df.empty:\n",
        "    print(\"All job adverts found and saved to CSV:\")\n",
        "    #print(all_jobs_df.head())\n",
        "    today_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")  # Get today's date in YYYY-MM-DD format\n",
        "    csv_filename = f\"wecare_jobs_webscraped_{today_date}.csv\"  # Include date in filename\n",
        "    all_jobs_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"Data saved to {csv_filename}\")\n",
        "else:\n",
        "    print(\"No job adverts found on any page, or an error occurred.\")\n",
        "\n",
        "\n",
        "print(\".....................\")\n",
        "print(\"Process ended\")"
      ],
      "metadata": {
        "id": "zT_LaGRA7HZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536052b4-01d0-4294-bd06-886ae5860f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "webscraping of https://wecare.wales/jobs/results/\n",
            "There are a total of 269 job adverts posted\n",
            "There are 23 pages to retrieve\n",
            ".....................\n",
            "Webscraping started\n",
            "Scraping data from: https://wecare.wales/jobs/results/\n",
            "Scraping data from: https://wecare.wales/jobs/results/p2\n",
            "Scraping data from: https://wecare.wales/jobs/results/p3\n",
            "Scraping data from: https://wecare.wales/jobs/results/p4\n",
            "Scraping data from: https://wecare.wales/jobs/results/p5\n",
            "Scraping data from: https://wecare.wales/jobs/results/p6\n",
            "Scraping data from: https://wecare.wales/jobs/results/p7\n",
            "Scraping data from: https://wecare.wales/jobs/results/p8\n",
            "Scraping data from: https://wecare.wales/jobs/results/p9\n",
            "Scraping data from: https://wecare.wales/jobs/results/p10\n",
            "Scraping data from: https://wecare.wales/jobs/results/p11\n",
            "Scraping data from: https://wecare.wales/jobs/results/p12\n",
            "Scraping data from: https://wecare.wales/jobs/results/p13\n",
            "Scraping data from: https://wecare.wales/jobs/results/p14\n",
            "Scraping data from: https://wecare.wales/jobs/results/p15\n",
            "Scraping data from: https://wecare.wales/jobs/results/p16\n",
            "Scraping data from: https://wecare.wales/jobs/results/p17\n",
            "Scraping data from: https://wecare.wales/jobs/results/p18\n",
            "Scraping data from: https://wecare.wales/jobs/results/p19\n",
            "Scraping data from: https://wecare.wales/jobs/results/p20\n",
            "Scraping data from: https://wecare.wales/jobs/results/p21\n",
            "Scraping data from: https://wecare.wales/jobs/results/p22\n",
            "Scraping data from: https://wecare.wales/jobs/results/p23\n",
            ".....................\n",
            "Webscraping completed\n",
            ".....................\n",
            "Creation of CSV file\n",
            "All job adverts found and saved to CSV:\n",
            "Data saved to wecare_jobs_webscraped_2025-04-22.csv\n",
            ".....................\n",
            "Process ended\n"
          ]
        }
      ]
    }
  ]
}